{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "limit = 10\n",
    "\n",
    "DATASET = \"./quora\"\n",
    "RESULTS_FILE = f\"/Users/acer/Desktop/thesis/results/{DATASET}/bm42.jsonl\"\n",
    "QUERIES = f\"/Users/acer/Desktop/thesis/datasets/{DATASET}/queries.jsonl\"\n",
    "QRELS = f\"/Users/acer/Desktop/thesis/datasets/{DATASET}/test.tsv\"\n",
    "RESULTS_FILE_BASELINE = f\"/Users/acer/Desktop/thesis/results/{DATASET}/bm25.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotated queries with relevance info\n",
    "def load_queries(_queries, qrels):\n",
    "    queries = {}\n",
    "    with open(_queries, \"r\") as file:\n",
    "        for line in file:\n",
    "            row = json.loads(line)\n",
    "            queries[str(row[\"_id\"])] = {**row, \"doc_ids\": []}\n",
    "\n",
    "    with open(qrels, \"r\") as file:\n",
    "        next(file) \n",
    "        for line in file:\n",
    "            query_id, doc_id, score = line.strip().split(\"\\t\")\n",
    "            if int(score):\n",
    "                queries[query_id][\"doc_ids\"].append(doc_id)\n",
    "\n",
    "    return {qid: q for qid, q in queries.items() if len(q[\"doc_ids\"]) > 0}\n",
    "\n",
    "# NDCG@K implementation this is just for binary relevance 0 or 1\n",
    "def ndcg_at_k(retrieved, relevant, k=limit):\n",
    "    dcg = 0.0                                                       # this is the discounted cumulative gain score\n",
    "    for i, doc_id in enumerate(retrieved[:k]):                      # iterate over the docs in retrieved and get index rank for each doc\n",
    "        if doc_id in relevant:                                      # check that document is in relevant\n",
    "            dcg += 1 / math.log2(i + 2)                             # compute dcg \n",
    "    ideal_hits = min(len(relevant), k)                              # compute how many hits you can get in a list of k documents\n",
    "    idcg = sum(1 / math.log2(i + 2) for i in range(ideal_hits))     # this is the best dcg you can get if everything was 100%\n",
    "    return dcg / idcg if idcg > 0 else 0                            # this is the normalized dcg against idcg\n",
    "\n",
    "\n",
    "\n",
    "def get_ndcg2id(_loaded_queries, res_file):\n",
    "\n",
    "    # Containers for evaluation metrics\n",
    "    n = 0        # total number of relevant documents across queries\n",
    "    hits = 0     # total number of relevant documents retrieved\n",
    "    num_queries = 0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    ndcg2id = {}\n",
    "\n",
    "    # Evaluation loop over each query's result\n",
    "    with open(res_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            query_id = str(entry[\"query_id\"])\n",
    "\n",
    "            if query_id not in queries:\n",
    "                continue\n",
    "\n",
    "            found_ids = [str(doc[\"doc_id\"]) for doc in entry[\"results\"][:limit]]\n",
    "            relevant = set(queries[query_id][\"doc_ids\"])\n",
    "\n",
    "            # Compute metrics per query\n",
    "            query_hits = len(set(found_ids) & relevant)\n",
    "            precision = query_hits / limit\n",
    "            recall = query_hits / len(relevant)\n",
    "            ndcg = ndcg_at_k(found_ids, relevant, k=limit)\n",
    "            ndcg2id[query_id] = ndcg\n",
    "            \n",
    "            # Append metric values to lists\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            ndcgs.append(ndcg)\n",
    "            hits += query_hits\n",
    "            n += len(relevant)\n",
    "            num_queries += 1\n",
    "\n",
    "    print(f\"Average NDCG@1: {sum(ndcgs) / len(ndcgs):.4f} for {res_file}\")\n",
    "    return ndcg2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "queries = load_queries(_queries=QUERIES, qrels=QRELS)\n",
    "print(\"BM42:\")\n",
    "scores_bm42 = get_ndcg2id(queries,res_file=RESULTS_FILE)\n",
    "print(\"BM25:\")\n",
    "scores_baseline = get_ndcg2id(queries,res_file=RESULTS_FILE_BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETS YOU SCORES AND IDS FOR BM25\n",
    "\n",
    "id_baseline = []\n",
    "score_baseline = []\n",
    "for ids, scores in scores_baseline.items():\n",
    "    id_baseline.append(ids)\n",
    "    score_baseline.append(scores)\n",
    "\n",
    "#GETS YOU SCORES AND IDS FOR BM42\n",
    "    \n",
    "id_bm42 = []\n",
    "score_bm42 = []\n",
    "for ids, scores in scores_bm42.items():\n",
    "    id_bm42.append(ids)\n",
    "    score_bm42.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETS THE INDIVIDUAL GAP SCORES BETWEEN TWO MODELS\n",
    "\n",
    "gap_score = []\n",
    "scores_dict = {}\n",
    "\n",
    "for i in range(len(id_bm42)):\n",
    "    gap = score_bm42[i] - score_baseline[i]         # THIS IS THE GAP SCORE BETWEEN BM25 AND BM42\n",
    "    sum = score_bm42[i] + score_baseline[i]         # THIS IS THEIR SUM SCORE \n",
    "    scores_dict[id_bm42[i]] = {\n",
    "        \"baseline\": score_baseline[i],\n",
    "        \"bm42\": score_bm42[i],\n",
    "        \"score_gap\": gap,\n",
    "        \"sum\": sum\n",
    "    }\n",
    "    gap_score.append(gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORES SORTED BASED ON WHAT YOU NEED TO BE LOOKING AT\n",
    "worse_bm42 = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1][\"score_gap\"])}\n",
    "better_bm42 = {k: v for k, v in sorted(scores_dict.items(), key=lambda item: item[1][\"score_gap\"], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in better_bm42.items():\n",
    "    print(i,d)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
